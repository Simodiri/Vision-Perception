{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fast_Style_Transfer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMX7bSHF1MtyWpSeveVZ0hi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Simodiri/Vision-Perception/blob/main/Fast_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing modules\n",
        "Import all necessary modules:\n",
        "\n",
        "*  `numpy`: work with arrays\n",
        "*  `tensorflow`: tensor operations\n",
        "*  `tensorflow.keras`: creating neural networks\n",
        "*  `pillow`: converting an image to a numpy array and viceversa`\n",
        "*  `time`: calculating time of each iteration\n",
        "*  `matplotlib`:displaying images and graphs in notebook\n",
        "*  `request`,`base64`,`io`: downloading and loading images from url\n",
        "* `os`: operating system level commands\n"
      ],
      "metadata": {
        "id": "r67vGPhBd2SX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import vgg19\n",
        "from tensorflow.keras.models import load_model,Model\n",
        "from PIL import Image\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import requests\n",
        "import base64\n",
        "import os\n",
        "from pathlib import Path\n",
        "from io import BytesIO\n",
        "matplotlib.rcParams['figure.figsize'] = (12,12)\n",
        "matplotlib.rcParams['axes.grid'] = False"
      ],
      "metadata": {
        "id": "0-u0akn9d8hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define utility functions"
      ],
      "metadata": {
        "id": "dhLTMohagU5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- *load_image* that is used to load image path and then convert it into a numpy array"
      ],
      "metadata": {
        "id": "7jPXcaW3gboz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_path, dim=None, resize=False):\n",
        "    img= Image.open(image_path)\n",
        "    if dim:\n",
        "        if resize:\n",
        "            img=img.resize(dim)\n",
        "        else:\n",
        "            img.thumbnail(dim)\n",
        "    img= img.convert(\"RGB\")\n",
        "    return np.array(img)"
      ],
      "metadata": {
        "id": "fAXfo-GHgn7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- *load_url_image* which loads images from url and converts into a numpy array"
      ],
      "metadata": {
        "id": "KkywYpW7gtIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_url_image(url,dim=None,resize=False):\n",
        "    img_request=requests.get(url)\n",
        "    img= Image.open(BytesIO(img_request.content))\n",
        "    if dim:\n",
        "        if resize:\n",
        "            img=img.resize(dim)\n",
        "        else:\n",
        "            img.thumbnail(dim)\n",
        "    img= img.convert(\"RGB\")\n",
        "    return np.array(img)"
      ],
      "metadata": {
        "id": "8WmZlP9dg0eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- *array_to_img* converts an array to an image"
      ],
      "metadata": {
        "id": "oCj7EE_Pg65U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def array_to_img(array):\n",
        "    array=np.array(array,dtype=np.uint8)\n",
        "    if np.ndim(array)>3:\n",
        "        assert array.shape[0]==1\n",
        "        array=array[0]\n",
        "    return Image.fromarray(array)"
      ],
      "metadata": {
        "id": "dmIrc3NYhCni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- *show_image* plots a single image"
      ],
      "metadata": {
        "id": "U3n8_KwwhE2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(image,title=None):\n",
        "    if len(image.shape)>3:\n",
        "        image=tf.squeeze(image,axis=0)\n",
        "    plt.imshow(image)\n",
        "    if title:\n",
        "        plt.title=title"
      ],
      "metadata": {
        "id": "pyc5yQ3Whb7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-*plot_images_grid* plots batches of images in grid"
      ],
      "metadata": {
        "id": "5Zqz_dJmhgxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images_grid(images,num_rows=1):\n",
        "    n=len(images)\n",
        "    if n > 1:\n",
        "        num_cols=np.ceil(n/num_rows)\n",
        "        fig,axes=plt.subplots(ncols=int(num_cols),nrows=int(num_rows))\n",
        "        axes=axes.flatten()\n",
        "        fig.set_size_inches((15,15))\n",
        "        for i,image in enumerate(images):\n",
        "            axes[i].imshow(image)\n",
        "    else:\n",
        "        plt.figure(figsize=(10,10))\n",
        "        plt.imshow(images[0])"
      ],
      "metadata": {
        "id": "oDFtt5xIhnPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create the fast style transfer\n",
        "The training model is an encoder-decoder architecture that has residual layes. The output has the same size of the input and spits the generated image. This model is trained on a loss called **perceptual loss**, which is used when the task is to compare two different images that look similar and it is used to find content and style discrepancies between those images.\n",
        "In order to feed the training model, a dataset of different images is used and in this case it's the **coco dataset**, which contains 328k images. In this code the dataset **kaggle challenge dataset** which has different images of landscapes. It's obvious that also a style image will be needed in order to learn its style using the autoencoder.\n",
        "\n",
        "The process of how this fast style transfer works is described in this image: \\\n",
        "**METTI L'IMMAGINE** (cambia il nome della rete nell'immagine) \\\n",
        "For training the model, batch of input training images are sent into the autoencoder, which provides an output that will be the *styled image*. While training, the output images batches into the loss model (in this case it's the vgg19) and features from different layers are extracted. The aim of this features is to calculate style loss and content loss, whose sum produce the perceptual loss mentioned before that trains the network.\n",
        "\n",
        "The main highlights of network:\n",
        "\n",
        "* Residual Layers\n",
        "* Encoder Decoder Model\n",
        "* output from decoder is passed to loss model(VGG) to calculate loss\n",
        "* training needs compute as these images are being passed to two networks on every step\n",
        "\n"
      ],
      "metadata": {
        "id": "J3mRpNh9iJ3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compute Loss\n",
        "A pretrained model calculates style loss and content loss, here the **vgg19**(vedere se cambiarlo)."
      ],
      "metadata": {
        "id": "InI4EfcSbvXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg=vgg19.VGG19(weights='imagenet',include_top=False)\n",
        "vgg.summary()"
      ],
      "metadata": {
        "id": "BlHAePMfbuJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then define the layers:"
      ],
      "metadata": {
        "id": "ylsUZ0l_cXkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content_layers=['block4_conv2']\n",
        "\n",
        "style_layers=['block1_conv1',\n",
        "            'block2_conv1',\n",
        "            'block3_conv1',\n",
        "            'block4_conv1',\n",
        "            'block5_conv1']"
      ],
      "metadata": {
        "id": "j0vIkVWicbif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets define a class that creates loss model with some additional methods for accessing feature maps from network."
      ],
      "metadata": {
        "id": "OYBGeUjvdBBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LossModel:\n",
        "  def __init__(self,pretrained_model,content_layers,style_layers):\n",
        "    self.model=pretrained_model\n",
        "    self.content_layers=content_layers\n",
        "    self.style_layers=style_layers\n",
        "    self.loss_model=self.get_model()\n",
        "\n",
        "  def get_model(self):\n",
        "    self.model.trainable=False\n",
        "    layers_names=self.style_layers+self.content_layers\n",
        "    outputs=[self.model.get_layer(name).output for name in layer_names]\n",
        "    new_model=Model(inputs=self.model.input,outputs=outputs)\n",
        "    return new_model\n",
        "\n",
        "  def get_activations(self,inputs):\n",
        "    inputs=inputs*255.0\n",
        "    style_length=len(self.style_layers)\n",
        "    outputs=self.loss_model(vgg19.preprocess_input(inputs))\n",
        "    style_output,content_output=outputs[:style_length],outputs[style_length:]\n",
        "    content_dict={name:value for name,value in zip(self.content_layers,content_output)}\n",
        "    style_dict={name:value for name,value in zip(self.stle_layers,style_output)}\n",
        "    return {'content':content_dict,'style':style_dict}\n",
        "    "
      ],
      "metadata": {
        "id": "lnbLWNRQdAu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create the loss model using the class defined before:"
      ],
      "metadata": {
        "id": "h5bDgY3Vi93e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_model = LossModel(vgg, content_layers, style_layers)"
      ],
      "metadata": {
        "id": "ZO5uICrejEQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to calculate the two types of losses (content and style), two functions are made:"
      ],
      "metadata": {
        "id": "rtuy7rHnj3_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def content_loss(placeholder,content,weight):\n",
        "    assert placeholder.shape == content.shape\n",
        "    return weight*tf.reduce_mean(tf.square(placeholder-content))\n",
        "\n",
        "def gram_matrix(x):\n",
        "    gram=tf.linalg.einsum('bijc,bijd->bcd', x, x)\n",
        "    return gram/tf.cast(x.shape[1]*x.shape[2]*x.shape[3],tf.float32)\n",
        "\n",
        "def style_loss(placeholder,style, weight):\n",
        "    assert placeholder.shape == style.shape\n",
        "    s=gram_matrix(style)\n",
        "    p=gram_matrix(placeholder)\n",
        "    return weight*tf.reduce_mean(tf.square(s-p))"
      ],
      "metadata": {
        "id": "AO4qrGryk1xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **percentual loss** is computed with weighted averaging of these losses:"
      ],
      "metadata": {
        "id": "rDF5DLfUk-KO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preceptual_loss(predicted_activations,content_activations,\n",
        "                    style_activations,content_weight,style_weight,\n",
        "                    content_layers_weights,style_layer_weights):\n",
        "    pred_content = predicted_activations[\"content\"]\n",
        "    pred_style = predicted_activations[\"style\"]\n",
        "    c_loss = tf.add_n([content_loss(pred_content[name],content_activations[name],\n",
        "                                  content_layers_weights[i]) for i,name in enumerate(pred_content.keys())])\n",
        "    c_loss = c_loss*content_weight\n",
        "    s_loss = tf.add_n([style_loss(pred_style[name],style_activations[name],\n",
        "                                style_layer_weights[i]) for i,name in enumerate(pred_style.keys())])\n",
        "    s_loss = s_loss*style_weight\n",
        "    return c_loss+s_loss"
      ],
      "metadata": {
        "id": "fPdr3ePvlJyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating the autoencoder\n",
        "In order to create an autoencoder, it will be needed:\n",
        "* `ReflectionPadding2D`: aplly reflection padding to images in convolutional networks\n",
        "* `InstanceNormalization`: normalizes inputs across channel\n",
        "* `ConvLayer`:  combine the three classes mentioned before\n",
        "* `ResidualLayer`: residual layer with two ConvLayer Blocks \n",
        "* `UpsampleLayer`: upsample the bottleneck representation in autoencoder (it acts as a deconvolution)."
      ],
      "metadata": {
        "id": "CtVe7g2kz_rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReflectionPadding2D(tf.keras.layers.Layer):\n",
        "    def __init__(self, padding=(1, 1), **kwargs):\n",
        "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
        "        self.padding = tuple(padding)\n",
        "    def call(self, input_tensor):\n",
        "        padding_width, padding_height = self.padding\n",
        "        return tf.pad(input_tensor, [[0,0], [padding_height, padding_height], \n",
        "                                     [padding_width, padding_width], [0,0] ], 'REFLECT')"
      ],
      "metadata": {
        "id": "ra7HFtqG_PA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InstanceNormalization(tf.keras.layers.Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(InstanceNormalization, self).__init__(**kwargs)\n",
        "    def call(self,inputs):\n",
        "        batch, rows, cols, channels = [i for i in inputs.get_shape()]\n",
        "        mu, var = tf.nn.moments(inputs, [1,2], keepdims=True)\n",
        "        shift = tf.Variable(tf.zeros([channels]))\n",
        "        scale = tf.Variable(tf.ones([channels]))\n",
        "        epsilon = 1e-3\n",
        "        normalized = (inputs-mu)/tf.sqrt(var + epsilon)\n",
        "        return scale * normalized + shift"
      ],
      "metadata": {
        "id": "-VEtWmPU_PfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self,filters,kernel_size,strides=1,**kwargs):\n",
        "        super(ConvLayer,self).__init__(**kwargs)\n",
        "        self.padding=ReflectionPadding2D([k//2 for k in kernel_size])\n",
        "        self.conv2d=tf.keras.layers.Conv2D(filters,kernel_size,strides)\n",
        "        self.bn=InstanceNormalization()\n",
        "    def call(self,inputs):\n",
        "        x=self.padding(inputs)\n",
        "        x=self.conv2d(x)\n",
        "        x=self.bn(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-Q08Ge1h_P2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self,filters,kernel_size,**kwargs):\n",
        "        super(ResidualLayer,self).__init__(**kwargs)\n",
        "        self.conv2d_1=ConvLayer(filters,kernel_size)\n",
        "        self.conv2d_2=ConvLayer(filters,kernel_size)\n",
        "        self.relu=tf.keras.layers.ReLU()\n",
        "        self.add=tf.keras.layers.Add()\n",
        "    def call(self,inputs):\n",
        "        residual=inputs\n",
        "        x=self.conv2d_1(inputs)\n",
        "        x=self.relu(x)\n",
        "        x=self.conv2d_2(x)\n",
        "        x=self.add([x,residual])\n",
        "        return x"
      ],
      "metadata": {
        "id": "68pdsQhVA_QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpsampleLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self,filters,kernel_size,strides=1,upsample=2,**kwargs):\n",
        "        super(UpsampleLayer,self).__init__(**kwargs)\n",
        "        self.upsample=tf.keras.layers.UpSampling2D(size=upsample)\n",
        "        self.padding=ReflectionPadding2D([k//2 for k in kernel_size])\n",
        "        self.conv2d=tf.keras.layers.Conv2D(filters,kernel_size,strides)\n",
        "        self.bn=InstanceNormalization()\n",
        "    def call(self,inputs):\n",
        "        x=self.upsample(inputs)\n",
        "        x=self.padding(x)\n",
        "        x=self.conv2d(x)\n",
        "        return self.bn(x)"
      ],
      "metadata": {
        "id": "BOmM_c8_BCZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having these classes created, the autoencoder will have this architecture:\n",
        "* 3 ConvLayer\n",
        "* 5 ResidualLayer\n",
        "* 3 UpsampleLayer"
      ],
      "metadata": {
        "id": "TXexXPc-BFvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StyleTransferModel(tf.keras.Model):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(StyleTransferModel, self).__init__(name='StyleTransferModel',**kwargs)\n",
        "        self.conv2d_1= ConvLayer(filters=32,kernel_size=(9,9),strides=1,name=\"conv2d_1_32\") #first three conv layers, dobling the filters\n",
        "        self.conv2d_2= ConvLayer(filters=64,kernel_size=(3,3),strides=2,name=\"conv2d_2_64\")\n",
        "        self.conv2d_3= ConvLayer(filters=128,kernel_size=(3,3),strides=2,name=\"conv2d_3_128\")\n",
        "        self.res_1=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_1_128\")\n",
        "        self.res_2=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_2_128\")\n",
        "        self.res_3=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_3_128\")\n",
        "        self.res_4=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_4_128\")\n",
        "        self.res_5=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_5_128\")\n",
        "        self.deconv2d_1= UpsampleLayer(filters=64,kernel_size=(3,3),name=\"deconv2d_1_64\")\n",
        "        self.deconv2d_2= UpsampleLayer(filters=32,kernel_size=(3,3),name=\"deconv2d_2_32\")\n",
        "        self.deconv2d_3= ConvLayer(filters=3,kernel_size=(9,9),strides=1,name=\"deconv2d_3_3\")\n",
        "        self.relu=tf.keras.layers.ReLU()\n",
        "    def call(self, inputs):\n",
        "        x=self.conv2d_1(inputs)\n",
        "        x=self.relu(x) #use relu as activation function\n",
        "        x=self.conv2d_2(x)\n",
        "        x=self.relu(x)\n",
        "        x=self.conv2d_3(x)\n",
        "        x=self.relu(x)\n",
        "        x=self.res_1(x)\n",
        "        x=self.res_2(x)\n",
        "        x=self.res_3(x)\n",
        "        x=self.res_4(x)\n",
        "        x=self.res_5(x)\n",
        "        x=self.deconv2d_1(x)\n",
        "        x=self.relu(x)\n",
        "        x=self.deconv2d_2(x)\n",
        "        x=self.relu(x)\n",
        "        x=self.deconv2d_3(x)\n",
        "        x = (tf.nn.tanh(x) + 1) * (255.0 / 2)\n",
        "        return x\n",
        "    \n",
        "    ## used to print shapes of each layer to check if input shape == output shape\n",
        "   \n",
        "    def print_shape(self,inputs):\n",
        "        print(inputs.shape)\n",
        "        x=self.conv2d_1(inputs)\n",
        "        print(x.shape)\n",
        "        x=self.relu(x)\n",
        "        x=self.conv2d_2(x)\n",
        "        print(x.shape)\n",
        "        x=self.relu(x)\n",
        "        x=self.conv2d_3(x)\n",
        "        print(x.shape)\n",
        "        x=self.relu(x)\n",
        "        x=self.res_1(x)\n",
        "        print(x.shape)\n",
        "        x=self.res_2(x)\n",
        "        print(x.shape)\n",
        "        x=self.res_3(x)\n",
        "        print(x.shape)\n",
        "        x=self.res_4(x)\n",
        "        print(x.shape)\n",
        "        x=self.res_5(x)\n",
        "        print(x.shape)\n",
        "        x=self.deconv2d_1(x)\n",
        "        print(x.shape)\n",
        "        x=self.relu(x)\n",
        "        x=self.deconv2d_2(x)\n",
        "        print(x.shape)\n",
        "        x=self.relu(x)\n",
        "        x=self.deconv2d_3(x)\n",
        "        print(x.shape)"
      ],
      "metadata": {
        "id": "EVWV_O7NCaLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the input shape and batch size:"
      ],
      "metadata": {
        "id": "lcej-4bIKCd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape=(256,256,3)\n",
        "batch_size=4"
      ],
      "metadata": {
        "id": "ObDe2cLnKF79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creat style model using the `StyleTransferModel` created before:"
      ],
      "metadata": {
        "id": "8TAFiX9ZKIZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "style_model = StyleTransferModel()\n",
        "\n",
        "style_model.print_shape(tf.zeros(shape=(1,*input_shape))) #check input shape and output shape"
      ],
      "metadata": {
        "id": "1xJ1ecp9KU01"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}